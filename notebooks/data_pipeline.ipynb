{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be1e9b90-c3c6-4dc6-a23c-313dd3e3922a",
   "metadata": {},
   "source": [
    "# Data Pipeline\n",
    "\n",
    "This notebook contains the initial wrangling and preprocessing of the data provided. We read the data straight into spark, where we do the computations, and write it to HDFS. That way the useful data is easily accessed in our main notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf4348-d9a4-4b81-845d-b56407cef7e4",
   "metadata": {},
   "source": [
    "## Initalize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13705c8-131e-43b4-a6dc-ea1004367ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local \n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "username = os.environ['RENKU_USERNAME']\n",
    "hiveaddr = os.environ['HIVE_SERVER2']\n",
    "(hivehost,hiveport) = hiveaddr.split(':')\n",
    "print(\"Operating as: {0}\".format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceebe416-df55-4ca1-81f6-e14710718c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b38aa48-8a19-4bac-af73-9b6a90d08bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local \n",
    "from IPython import get_ipython\n",
    "# Set up spark session\n",
    "server = 'http://iccluster029.iccluster.epfl.ch:8998'\n",
    "\n",
    "packages = \"\"\"{\"packages\": \"graphframes:graphframes:0.6.0-spark2.3-s_2.11\"}\"\"\"\n",
    "\n",
    "# Set application name as \"<your_gaspar_id>-final_project\"\n",
    "get_ipython().run_cell_magic(\n",
    "    'spark',\n",
    "    line='config', \n",
    "    cell=f\"\"\"{{ \"name\": \"{username}-final_project\", \"executorMemory\": \"4G\", \"executorCores\": 4, \"numExecutors\": 10, \"driverMemory\": \"4G\", \"conf\": {packages}}}\"\"\"\n",
    ")\n",
    "# Send username to spark channel\n",
    "get_ipython().run_line_magic(\n",
    "    \"spark\", \"add -s {0}-final_project -l python -u {1} -k\".format(username, server)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc008884-248d-429d-97a1-857c60bd8f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We are using Spark %s' % spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126ec3a0-b2f0-4c76-a1c4-ca54696db89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866c0e5f-7908-4d87-ac00-4dbfabd7e90f",
   "metadata": {},
   "source": [
    "## Stops and walking connections\n",
    "\n",
    "We start by loading all the stops in Switzerland, followed by extracting those in the Zurich area. These are then used to derive the walking connections, based on the rules given in the project destription. Therefore, we do not rely on the transfers contained in `transfers.txt`\n",
    "\n",
    "\n",
    "**Drawbacks**:\n",
    "- Walking time calculation does only consider straight line distance between two points. Using `transfers.txt` would likely give more accurate measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602af53c-a4d5-44f2-afde-8ef7ad76b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark \n",
    "# Load stops data\n",
    "df = spark.read.orc('/data/sbb/orc/allstops')\n",
    "df.printSchema()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b7bc1-fa0c-412d-9452-74a87c57ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# Extract coordinates of Zurich HB as CENTER\n",
    "ZURICH_HB = df[df['stop_id'] == '8503000'].first()\n",
    "CENTER = (F.lit(ZURICH_HB.stop_lat), F.lit(ZURICH_HB.stop_lon))\n",
    "\n",
    "# Haversine distance function\n",
    "def dist(lat_x, lon_x, lat_y=CENTER[0], lon_y=CENTER[1]):\n",
    "    return F.acos(\n",
    "        F.sin(F.radians(lat_x)) * F.sin(F.radians(lat_y)) +\n",
    "        F.cos(F.radians(lat_x)) * F.cos(F.radians(lat_y)) *\n",
    "        F.cos(F.radians(lon_x) - F.radians(lon_y))\n",
    "    ) * F.lit(6371.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d54d9-be89-457c-a4b2-dd27e5d5822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# Extract relevant stops in Zurich area\n",
    "\n",
    "stops = (\n",
    "    df.withColumn('distance',\n",
    "                  dist('stop_lat', 'stop_lon'))\n",
    "      .filter('distance <= 15')\n",
    "      .drop('location_type', 'parent_station')\n",
    ")\n",
    "stops.show()\n",
    "stops.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12db38b4-3574-4460-b1a6-76d82a553eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving walking connections\n",
    "\n",
    "source = stops.select(F.col('stop_id').alias('source_id'), \n",
    "                      F.col('stop_lat').alias('src_lat'), \n",
    "                      F.col('stop_lon').alias('src_lon')\n",
    "                     )\n",
    "destination = stops.select(F.col('stop_id').alias('destination_id'), \n",
    "                     F.col('stop_lat').alias('dest_lat'), \n",
    "                     F.col('stop_lon').alias('dest_lon')\n",
    "                    )\n",
    "\n",
    "# Cross join stops with itself, remove stops that are joined with itself,\n",
    "# calculate distance, remove stops that are too far away.\n",
    "# Finally calculate the walking time, round up to closest minute and\n",
    "# convert to seconds (to get it on same format as transport connections)\n",
    "\n",
    "walking_connections = (\n",
    "    source.crossJoin(destination)\n",
    "          .filter('source_id <> destination_id')\n",
    "          .withColumn('distance', \n",
    "                      dist('src_lat', 'src_lon', 'dest_lat', 'dest_lon'))\n",
    "          .filter('distance <= 0.5')\n",
    "          .withColumn('travel_time',\n",
    "                      F.ceil(F.col('distance') / F.lit(0.05)) * F.lit(60))\n",
    "          .drop('src_lat', 'src_lon', 'dest_lat', 'dest_lon', 'distance')\n",
    ")\n",
    "walking_connections.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cff203-e755-431d-bb82-62bea505448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write stops and walking connections to HDFS\n",
    "stops.write.orc('/group/gutane/stops', mode='overwrite')\n",
    "walking_connections.write.orc('/group/gutane/walking_connections', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c0144-5e1a-45e4-a444-b35c119ce3fb",
   "metadata": {},
   "source": [
    "## Trip connections\n",
    "\n",
    "We only view trips that occur between 6AM and 12PM each day, as we consider these reasonable times. As we want a product that can give a trip any arbitrary weekday, we keep only trips that are scheduled every day. In addition, we only keep trips where all stops are within the Zurich area to simplify the search process. Due to this, there will be some stops in the area which are not reachable from the connections in the resulting dataset. This is handled in the `final_project.ipynb`\n",
    "\n",
    "We transform the format of the data from keeping on entry per stop on a trip, to keep on entry per connection, that is a row contains both the current stop and the next stop on a trip.\n",
    "\n",
    "**Drawbacks**:\n",
    "- 6AM and 12PM doesn't capture all trips\n",
    "- Only looking at trips where all stops are in Zurich area reduces expressiveness of model and makes some legal stops unreachable\n",
    "- Ideally we would keep track of trips per day, but to keep things simple in our initial product, we only view trips that occur every weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e3628-2d18-439c-8b92-57321057c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Retrieving the last week of stop_times as this is the most relevant\n",
    "stop_times = spark.read.csv('/data/sbb/csv/stop_times/2020/12/09/stop_times.txt', header=True)\n",
    "stop_times.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814a3595-ea3f-4b70-9ba8-0ee91b631687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out 'unreasonable' times of day\n",
    "# We choose to look at trips between 6AM and 12PM\n",
    "# The filter works because string representation of dates preserves the ordering\n",
    "\n",
    "stop_times = (\n",
    "    stop_times.filter('arrival_time >= \"06:00:00\" and departure_time >= \"06:00:00\"')\n",
    "              .filter('arrival_time <= \"23:59:59\" and departure_time <= \"23:59:59\"')\n",
    "              .withColumn('arrival_time', F.to_timestamp('arrival_time'))\n",
    "              .withColumn('departure_time', F.to_timestamp('departure_time'))\n",
    "              .withColumn('stop_sequence', F.col('stop_sequence').cast('int'))\n",
    ")\n",
    "stop_times.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d903f07-4d92-402f-a572-a43d49848c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For efficiency, we collect all legal stop id's in a set, which yield \n",
    "# O(1) lookup\n",
    "legal_stops = set([row.stop_id for row in stops.select('stop_id').collect()])\n",
    "\n",
    "# Define window over each trip_id, ordered by stop_sequence\n",
    "window = (\n",
    "    Window.partitionBy('trip_id')\n",
    "          .orderBy(F.asc('stop_sequence'))\n",
    ")\n",
    "\n",
    "# Although a trip is legal as long as there are at least two legal stops in the trip, \n",
    "# we choose for simplicity reasons to only include trips where all stops are legal\n",
    "# Altough this reduces the the expressiveness of our model, we remove a tricky edge case\n",
    "\n",
    "# User defined spark sql function which takes a an iterable \n",
    "# of stop id's and returns true if all stops are in legal_stops\n",
    "@F.udf\n",
    "def is_legal(stop_ids): \n",
    "    masks = [int(stop in legal_stops) for stop in stop_ids]\n",
    "    return sum(masks) == len(masks)\n",
    "\n",
    "# Collect the list of all stops in each trip\n",
    "legal_mask = (\n",
    "    is_legal(F.collect_list('stop_id').over(window))\n",
    ")\n",
    "\n",
    "# Add is_legal mask to each trip\n",
    "stop_times = (\n",
    "    stop_times.select('trip_id', \n",
    "                      'arrival_time',\n",
    "                      'departure_time', \n",
    "                      'stop_id', \n",
    "                      'stop_sequence', \n",
    "                      legal_mask.alias('is_legal'))\n",
    ")\n",
    "\n",
    "# Filter on and drop is_legal column\n",
    "stop_times = stop_times.filter('is_legal == true').drop('is_legal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b090f1b7-98f4-4825-b3e2-a2e917fd4854",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_times.show(5)\n",
    "stop_times.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d742485-8e7b-47af-ab35-7b5ed0593762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata\n",
    "routes = spark.read.csv('/data/sbb/csv/routes/2020/12/09/routes.txt', header=True)\n",
    "trips = spark.read.csv('/data/sbb/csv/trips/2020/12/09/trips.txt', header=True)\n",
    "calendar = spark.read.csv('/data/sbb/csv/calendar/2020/12/09/calendar.txt', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f66d52-0158-4c65-99fc-3058d7a5dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The service must be valid in the given period. \n",
    "# Since the last end date in the dataset is 12:12:2020, we assume that every \n",
    "# entry with this end_date is still valid\n",
    "\n",
    "# Because our service should give the correct information for an arbitrary \n",
    "# normal business day (Monday to Friday), we choose to simplify our model\n",
    "# by only looking at trips that are valid each day. \n",
    "\n",
    "legal_services = (\n",
    "    calendar.filter('start_date <= \"20201209\"')\n",
    "            .filter('monday == \"1\" and tuesday == \"1\" and \\\n",
    "                     wednesday == \"1\" and thursday == \"1\" and friday == \"1\"')\n",
    "            .select('service_id')\n",
    ")\n",
    "legal_services.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b1a5d-995c-4c28-9a3e-aa17cbf2dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter relevant trips by extracting the unique trip_id's\n",
    "# from stop_times, and join with the newly loaded trips\n",
    "trip_ids = stop_times.select('trip_id').distinct()\n",
    "trips = trips.join(trip_ids, on='trip_id', how='inner')\n",
    "\n",
    "# One entry per legal trip_id\n",
    "trips.count() == stop_times.select('trip_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00acef7-0301-4a21-b024-2da0a815c202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append route information to each trip\n",
    "trips_with_route_info = trips.join(routes, on='route_id', how='inner')\n",
    "trips_metadata = trips_with_route_info.select('service_id', 'trip_id', 'route_short_name', 'route_desc')\n",
    "\n",
    "# And then only keep the trip id's corresponding to legal service groups\n",
    "trips_metadata = trips_metadata.join(legal_services, on='service_id', how='inner')\n",
    "trips_metadata.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d069faca-3c96-4fe2-8f11-bf2bab37ee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_metadata.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a571599-7067-48c9-98bf-3afda0002411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with stop_times to decrease number of connection\n",
    "print('Number of connections before filtering out invalid services: {}'.format(stop_times.count()))\n",
    "stop_times = stop_times.join(trips_metadata.select('trip_id'), on='trip_id', how='inner')\n",
    "print('Number of connections after filtering out invalid services: {}'.format(stop_times.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc11205a-0fb3-4fb3-95eb-789a30fbdc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "# To extract stops that are connected by a trip, we define a window function partitioned over each trip_id, \n",
    "# and ordered by the stop_sequence (ascending). That way we can access subsequent stops with F.lag\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = (\n",
    "    Window.partitionBy('trip_id')\n",
    "          .orderBy(F.asc('stop_sequence'))\n",
    ")\n",
    "\n",
    "# Column of each outgoing connection (same as stop_id)\n",
    "source = (\n",
    "    F.lag('stop_id', 0).over(window)\n",
    ")\n",
    "\n",
    "# Column of destination for outgoing connection\n",
    "destination = (\n",
    "    F.lag('stop_id', -1).over(window)\n",
    ")\n",
    "\n",
    "dest_arrival_time = (\n",
    "    F.lag('arrival_time', -1).over(window)\n",
    ")\n",
    "dest_departure_time = (\n",
    "    F.lag('departure_time', -1).over(window)\n",
    ")\n",
    "\n",
    "# Travel time from previous stop to next (for ML model)\n",
    "travel_time = (\n",
    "    (F.lag('arrival_time', 0).over(window).cast('long') - \n",
    "    F.lag('departure_time', 1).over(window).cast('long'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ce6525-389a-41a3-94ce-0f3bfa687632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all edges, plus meta data, in one dataframe\n",
    "# Important: \n",
    "    # travel_time refers to the time it took from the stop prior to source stop\n",
    "    # this is a predictive feature for delay model, as the time spent \n",
    "    # on the journey is more relevant for the delay at the stop than the time that will be spent\n",
    "connections = (\n",
    "    stop_times.select(\n",
    "        'trip_id', \n",
    "        source.alias('src_id'), \n",
    "        F.col('arrival_time').alias('src_arrival_time'), \n",
    "        F.col('departure_time').alias('src_departure_time'),\n",
    "        destination.alias('dest_id'),\n",
    "        dest_arrival_time.alias('dest_arrival_time'),\n",
    "        dest_departure_time.alias('dest_departure_time'),\n",
    "        'stop_sequence',\n",
    "        travel_time.alias('travel_time_from_prev'),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Total travel time for the trip after reaching source (for ML model)\n",
    "connections = (\n",
    "    connections.withColumn('cummulated_travel_time',\n",
    "                           F.sum('travel_time_from_prev')\n",
    "                            .over(window.rowsBetween(Window.unboundedPreceding, 0))\n",
    "                          )\n",
    ")\n",
    "\n",
    "# Add metadata so it doesn't have to be saved on separate file\n",
    "# We sacrifice normalization for convenience\n",
    "connections = connections.join(trips_metadata, on='trip_id', how='left_outer') \\\n",
    "                         .drop('service_id')\n",
    "\n",
    "connections.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf5acd7-4a33-4f05-9e34-734e2a90f55a",
   "metadata": {},
   "source": [
    "Which is a reasonable size for a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b92013-f33f-4dd8-aaf4-18b9ee510456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally write stop connections to file\n",
    "connections.write.orc('/group/gutane/connections', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908fe729-f81e-45a5-acfa-894afe6042c1",
   "metadata": {},
   "source": [
    "## Realtime data from Istdaten\n",
    "We keep only the rows where `AN_PROGNOSE_STATUS` and `AB_PROGNOSE_STATUS` are either \\\"REAL\\\", \\\"GESCHAETZT\\\" or \\\"PROGNOSE\\\", as these are the fields we´re interested in. When exploring the dataset, we found a lot of duplicate values. We identified (`trip_date`, `trip_id` and `scheduled_arrival_time`) to be the columns in which we wanted distinct values over, as there often where several values for `actual_arrival_time` per compbination of `trip_date` and `trip_id`. This caused trouble when calculating travel times between two stops. The transformation of the rows is done similarly to what we did with `stop_times.txt`\n",
    "\n",
    "**Drawbacks**:\n",
    "- After transforming the dataset to show subsequent stops in each trip, we found ~20k rows where the departure time from the previous stops was dated after the arrival time at the current stop. This resulted in negative travel times between stops. The rows we inspected revealed that these inconsistencies were caused by duplicated entries. We therefore just removed them. However, this assumption might not hold for every row, potentially resulting in inaccurate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcaff93-51bd-48aa-b4a8-e89b2cb393b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Retrieving the last week of stop_times as this is the most relevant\n",
    "\n",
    "# After extracting our columns of interest, we found a lot of duplicate values.\n",
    "# These made it hard to distinguish between different trips under the same trip_id. \n",
    "# In addition, there are sometimes several rows for the same stop, differing only in the \n",
    "# actual departure and arrival times\n",
    "# We therefore continue with only the distinct rows over trip_date, trip_id and scheduled_arrival_time\n",
    "\n",
    "istdaten = spark.read.orc('/data/sbb/orc/istdaten')\n",
    "\n",
    "istdaten = (\n",
    "    istdaten.filter('AN_PROGNOSE_STATUS == \"REAL\" or AN_PROGNOSE_STATUS == \"GESCHAETZT\" or AN_PROGNOSE_STATUS == \"PROGNOSE\"')\n",
    "            .filter('AB_PROGNOSE_STATUS == \"REAL\" or AB_PROGNOSE_STATUS == \"GESCHAETZT\" or AB_PROGNOSE_STATUS == \"PROGNOSE\"')\n",
    "            .select(F.to_timestamp('BETRIEBSTAG', 'dd.MM.yyyy').alias('trip_date'), \n",
    "                    F.col('FAHRT_BEZEICHNER').alias('trip_id'), \n",
    "                    F.col('PRODUKT_ID').alias('transport_type'),\n",
    "                    F.col('BPUIC').alias('stop_id'),\n",
    "                    F.to_timestamp('ANKUNFTSZEIT', 'dd.MM.yyyy HH:mm').alias('scheduled_arrival_time'),\n",
    "                    F.to_timestamp('AN_PROGNOSE', 'dd.MM.yyyy HH:mm:ss').alias('actual_arrival_time'),\n",
    "                    F.to_timestamp('ABFAHRTSZEIT', 'dd.MM.yyyy HH:mm').alias('scheduled_departure_time'),\n",
    "                    F.to_timestamp('AB_PROGNOSE', 'dd.MM.yyyy HH:mm:ss').alias('actual_departure_time')\n",
    "                   )\n",
    "            .dropDuplicates(['trip_date', 'trip_id', 'scheduled_arrival_time']) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68a7006-4cd3-496f-b362-b6900ba29952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to extract each trip, similar to the structure in GTFS data\n",
    "# Can then use the cummulated length and stop number as features\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define window over each trip_id, ordered by stop_sequence\n",
    "window = (\n",
    "    Window.partitionBy('trip_id', 'trip_date')\n",
    "          .orderBy(F.asc('scheduled_arrival_time'))\n",
    ")\n",
    "\n",
    "# Extract travel time\n",
    "travel_time = (\n",
    "    (F.lag('scheduled_arrival_time', -1).over(window).cast('long') - \n",
    "     F.lag('scheduled_departure_time', 0).over(window).cast('long'))\n",
    ")\n",
    "\n",
    "delay_departure = (\n",
    "    (F.lag('scheduled_departure_time', 1).over(window).cast('long') - \n",
    "     F.lag('actual_departure_time', 1).over(window).cast('long'))\n",
    ")\n",
    "\n",
    "# Select relevant columns\n",
    "istdaten = istdaten.select('trip_date', 'trip_id', 'transport_type', 'stop_id', 'scheduled_arrival_time',\n",
    "                           'actual_arrival_time', 'scheduled_departure_time', 'actual_departure_time', \n",
    "                           travel_time.alias('travel_time'), delay_departure.alias('delay_departure')\n",
    "                          )\n",
    "# After removing the duplicates, we found roughly 20k rows with negative travel times\n",
    "# After further inspections, these seemed to be faulty inserts in the middle of valid trips\n",
    "# We therefore assume this is the case for all, and remove all rows with negative travel times,\n",
    "# before calculating the cumulative sum\n",
    "\n",
    "istdaten = istdaten.filter('travel_time >= 0')\n",
    "\n",
    "istdaten = \\\n",
    "    istdaten.withColumn('cumulated_travel_time', \n",
    "                         F.sum('travel_time')\n",
    "                          .over(window.rowsBetween(Window.unboundedPreceding, 0))\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cfd6ab-cfbc-4cdd-a995-f5d626130440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where the arrival time is null, as we don't need\n",
    "# the first stop of each trip in the predictive model\n",
    "# We don't filter on departure times as this would remove the \n",
    "# the last stop in each trip, which we need for our model\n",
    "\n",
    "istdaten = istdaten.filter('actual_arrival_time is not null') \\\n",
    "                   .filter('scheduled_arrival_time is not null') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d3185-1f88-4a37-8ac8-aeb080be7936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally write to file\n",
    "istdaten.write.orc('/group/gutane/istdaten_distinct', mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
