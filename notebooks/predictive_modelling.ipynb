{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a3e936c-3beb-4d04-a33c-8b9a0e838ad0",
   "metadata": {},
   "source": [
    "## Predictive modelling\n",
    "\n",
    "This notebook contains our attempts at modelling and predicting delays. The notebook goes chornologically through our thought processes and how the modelling went. The final model used can be found in the final_project file, this notebook focuses on the different approaches we tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46c1c7-012c-4e7b-b4ad-e204c4e89737",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 50) \n",
    "pd.options.mode.chained_assignment = None\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "\n",
    "username = os.environ['RENKU_USERNAME']\n",
    "hiveaddr = os.environ['HIVE_SERVER2']\n",
    "(hivehost,hiveport) = hiveaddr.split(':')\n",
    "print(\"Operating as: {0}\".format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282d81e1-0f33-429f-900d-f8aecd378e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics\n",
    "# If using Python 3 kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ceecca-a128-4ef6-ba37-9065722da184",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local \n",
    "from IPython import get_ipython\n",
    "# Set up spark session\n",
    "server = 'http://iccluster029.iccluster.epfl.ch:8998'\n",
    "\n",
    "packages = \"\"\"{\"packages\": \"graphframes:graphframes:0.6.0-spark2.3-s_2.11\"}\"\"\"\n",
    "\n",
    "# Set application name as \"<your_gaspar_id>-final_project\"\n",
    "get_ipython().run_cell_magic(\n",
    "    'spark',\n",
    "    line='config', \n",
    "    cell=f\"\"\"{{ \"name\": \"{username}-final_project\", \"executorMemory\": \"4G\", \"executorCores\": 4, \"numExecutors\": 10, \"driverMemory\": \"4G\", \"conf\": {packages}}}\"\"\"\n",
    ")\n",
    "# Send username to spark channel\n",
    "get_ipython().run_line_magic(\n",
    "    \"spark\", \"add -s {0}-final_project -l python -u {1} -k\".format(username, server)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59672a49-1b62-47e1-9c33-c60f5c0d4bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93844ae-9e99-4362-876e-21c2418f5c8d",
   "metadata": {},
   "source": [
    "## Predictive model of arrival time\n",
    "Here, we build a predictive model for the delay in the transport network. First, we do the following assumption: It suffices to predict the arrival time. This is because we can then calculate the departure time as:\n",
    "\n",
    "max(plan_departure, pred_arrival + wait_time)\n",
    "\n",
    "Where wait_time is a parameter we decide upon through finding the median in the dataset for departures that are delayed. The reason that we do this assumption is that buses and trains scarcely get delayed on the station, but rather between stations. On station, there is more of a fixed time that is needed to lett passenger go on and off the transport."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809541d5-0ce0-4bd8-8473-82c08416b1fa",
   "metadata": {},
   "source": [
    "## Weather data\n",
    "\n",
    "We use weather data to strengthen our predictive model as it probably correlates with delays.\n",
    "\n",
    "The way we load and save the weather data below may seem strange. The reason for this is that our plan was to read it in lcoally and the use %%send_to_spark to get it into spark. However, the send_to_spark command created a seperate spark session which we only were able to access thorugh the %%pretty magic. We therefore decided to write it to HDFS and load it in again with the correct Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3bedc-e218-4917-b319-ff43309162ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "# reading in the weather csv\n",
    "weather_df = pd.read_csv('../data/weather.csv')\n",
    "weather_df = weather_df.fillna(weather_df.median())\n",
    "weather_df = weather_df.drop(columns='tsun')\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7644489-a59f-4782-9a89-0e6e56677921",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%send_to_spark -i weather_df -t df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb507861-8b6b-4fb8-86c4-476e83a9f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pretty\n",
    "# writing the file to the hdfs cluster\n",
    "weather_df.write.csv('/group/gutane/weather.csv', mode='overwrite', header='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a6e883-eb41-4ea0-b0d8-0bfeea98681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the weather file\n",
    "weather_df = spark.read.csv('/group/gutane/weather.csv', header='True')\n",
    "weather_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4d821c-5b8c-4fab-b51e-ec1b8398d9a3",
   "metadata": {},
   "source": [
    "## Istdaten data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f11e95e-7242-48fd-9d71-195e78f04c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "# Load distinct istdaten\n",
    "istdaten = spark.read.orc('/group/gutane/istdaten_distinct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a87cfe-7c12-4a83-8b20-f9707f844ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only extracting rows with stops inside the 15km radius\n",
    "istdaten = istdaten.filter(istdaten.stop_id.isin(uni_stops))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee87dfe-5815-456f-9d6f-b7772eb15dc3",
   "metadata": {},
   "source": [
    "## Preprocessing of the data\n",
    "\n",
    "To make the data ready for modelling, we need to complete som preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437ecae6-6ca0-4882-8eff-7f4dcce598d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# removing rows where actual and prognosed arrival time does not exist\n",
    "df = istdaten.filter('actual_arrival_time is not null')\n",
    "df = df.filter('scheduled_arrival_time is not null')\n",
    "\n",
    "# Calculating delay, the difference between actual and scheduled arrival time\n",
    "df = df.withColumn('actual_arrival_time',to_timestamp(col('actual_arrival_time'), \"dd.MM.yyyy HH:mm:ss\"))\\\n",
    "  .withColumn('scheduled_arrival_time', to_timestamp(col('scheduled_arrival_time'), \"dd.MM.yyyy HH:mm\"))\\\n",
    "  .withColumn('delay',unix_timestamp(\"actual_arrival_time\") - unix_timestamp('scheduled_arrival_time'))\n",
    "\n",
    "# Calculating delay, the difference between actual and scheduled arrival time\n",
    "df = df.withColumn('actual_departure_time',to_timestamp(col('actual_departure_time'), \"dd.MM.yyyy HH:mm:ss\"))\\\n",
    "  .withColumn('scheduled_departure_time', to_timestamp(col('scheduled_departure_time'), \"dd.MM.yyyy HH:mm\"))\\\n",
    "\n",
    "# adding weekday, month, year, and hour of the day to the table\n",
    "df = df.withColumn('trip_date',to_date(col('trip_date'), \"dd.MM.yyyy\"))\\\n",
    "  .withColumn('weekday', dayofweek('trip_date'))\\\n",
    "  .withColumn('month', month('trip_date'))\\\n",
    "  .withColumn('year', year('trip_date'))\\\n",
    "  .withColumn('time', hour('scheduled_arrival_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff9fb3-bc99-40fe-b086-2ecb3c3e40e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ebe89a-e459-4a79-9337-095ed6c2e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how common it is that delay is created at the station\n",
    "count = df.filter(unix_timestamp(df.scheduled_departure_time)>\n",
    "                  unix_timestamp(df.actual_departure_time)+unix_timestamp(df.scheduled_arrival_time)-unix_timestamp(df.actual_arrival_time) + 60)\\\n",
    "                  .count()\n",
    "print(\"In %.4f %% of the stops, moren that one minute delay is created at the station\"%(float(count)/float(total_count)*100))\n",
    "print(\"That is in %d of %d rows\"%(count, total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b1117c-7839-414d-b67a-be2d640a67ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing rows that do not null as travel time\n",
    "\n",
    "df = df.filter('travel_time is not null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1992b3d4-c22a-4e83-8892-d76f2eaa9db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting columns to correct datatype\n",
    "weather_df = weather_df.withColumn('trip_date',to_date(col('date'), \"dd/MM/yyyy\"))\\\n",
    "                        .withColumn('prcp', weather_df.prcp.cast('long'))\\\n",
    "                        .withColumn('pres', weather_df.pres.cast('long'))\\\n",
    "                        .withColumn('snow', weather_df.snow.cast('long'))\\\n",
    "                        .withColumn('tavg', weather_df.tavg.cast('long'))\\\n",
    "                        .withColumn('tmax', weather_df.tmax.cast('long'))\\\n",
    "                        .withColumn('tmin', weather_df.tmin.cast('long'))\\\n",
    "                        .withColumn('wdir', weather_df.wdir.cast('long'))\\\n",
    "                        .withColumn('wpgt', weather_df.wpgt.cast('long'))\\\n",
    "                        .withColumn('wspd', weather_df.wspd.cast('long'))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59603823-c959-41fe-84c6-29dc1d23fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging df and weather_df on date\n",
    "df_full = df.join(weather_df, 'trip_date', 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ac482-c60f-4b6d-8bcc-7f2e997e5ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the new schema\n",
    "df_full.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd45ec97-c3ca-4592-9795-fa2bae3115be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a sample of the data frame for testing of code\n",
    "sample = df_full.sample(0.001)\n",
    "# writing the sample to hdfs of speed purposes\n",
    "sample.write.orc('/group/gutane/sample', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f34fc2-32f7-4447-abde-0b23b8fac6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the sample in from the new partitions in hdfs\n",
    "sample = spark.read.orc('/group/gutane/sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c01303-1dd3-46fd-91b3-145ecc73fc16",
   "metadata": {},
   "source": [
    "## EDA and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7e1413-6fc9-4a00-9ad5-6d10835acf9b",
   "metadata": {},
   "source": [
    "_Which features should we use? - some initial thoughts_\n",
    "\n",
    "- PRODUCT_ID is probably smart as bus probably are more delayed than trains\n",
    "- STOP_ID, some stops may be more prone to delays than others, e.g. Lausanne-Flon vs. a small village in Valais.\n",
    "- Delay in departure at previous stop, this will probably be a good feature, but will be predicted by the model in real-time so I kinda doubt it will work out in practice. But worth a try.\n",
    "- DAY, more delay on Monday at 08 than Sunday at 08.\n",
    "- MONTH, More delay in the winter than in the summer?\n",
    "- TIME_OF_DAY, More delay in rush hour than the middle of the night\n",
    "- Weather, staff mentioned this as a previously succesful predictor. Makes sense that ice-->more delay, as buses slip etc. So temperature and snow for example could be good variables.\n",
    "- diff scheduled departure previous stop to scheduled arrival this stop, as distance between stops should impact delay times\n",
    "- cumulated trip time at this stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38037c5c-1135-4e7e-b041-359751978c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1416a1-1009-4f66-97ab-2b4894bb3c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# swiss holidays from 2018-2021\n",
    "swiss_holidays = [datetime.date(2018, 1, 1),datetime.date(2018, 4, 1), datetime.date(2018, 3, 30),\n",
    "                  datetime.date(2018, 4, 2), datetime.date(2018, 5, 10), datetime.date(2018, 5, 20),\n",
    "                  datetime.date(2018, 5, 21), datetime.date(2018, 8, 1), datetime.date(2018, 12, 25),\n",
    "                  datetime.date(2019, 1, 1), datetime.date(2019, 4, 21), datetime.date(2019, 4, 19),\n",
    "                  datetime.date(2019, 4, 22), datetime.date(2019, 5, 30), datetime.date(2019, 6, 9),\n",
    "                  datetime.date(2019, 6, 10), datetime.date(2019, 8, 1), datetime.date(2019, 12, 25),\n",
    "                  datetime.date(2020, 1, 1), datetime.date(2020, 4, 12), datetime.date(2020, 4, 10),\n",
    "                  datetime.date(2020, 4, 13), datetime.date(2020, 5, 21), datetime.date(2020, 5, 31),\n",
    "                  datetime.date(2020, 6, 1), datetime.date(2020, 8, 1), datetime.date(2020, 12, 25),\n",
    "                  datetime.date(2021, 1, 1), datetime.date(2021, 4, 4), datetime.date(2021, 4, 2),\n",
    "                  datetime.date(2021, 4, 5), datetime.date(2021, 5, 13), datetime.date(2021, 5, 23),\n",
    "                  datetime.date(2021, 5, 24), datetime.date(2021, 8, 1), datetime.date(2021, 12, 25)]\n",
    "\n",
    "# adding a variable specifying whether a given day is a holiday or not\n",
    "sample = sample.withColumn('holiday', sample.trip_date.isin(swiss_holidays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b609e25-5e46-4fe9-8d19-686c010cef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating # unique values in the feature columns\n",
    "features = ['transport_type','travel_time',\n",
    "            'weekday', 'month','stop_id', 'year', 'time',\n",
    "            'prcp', 'pres', 'snow', 'tavg', 'tmax', 'tmin',\n",
    "            'wdir', 'wpgt', 'wspd','holiday', 'delay_departure']\n",
    "label = 'delay'\n",
    "unique = sample.select([F.countDistinct(col).alias(col) for col in features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d27535-3884-4f29-bdf8-d3c586291716",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o unique -t df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ac576-3703-411b-ba9d-db04027289f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "# inspecting the number of unique values locally\n",
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff1c6d-8198-4c78-8738-c70c831d95c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating descriptive statistics for the dataframe\n",
    "statistics = sample.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5341413-b94a-4aea-ad64-b71aab711eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o statistics -t df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2316f-f411-4e35-818a-aa1f794a1ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "# inspecting descriptive statistics locally\n",
    "statistics.drop(columns = ['trip_id', 'stop_id', 'transport_type', 'date'], inplace=True)\n",
    "statistics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cef6e2-456b-48c5-a042-7b2f2096470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as seen above, we made some mistake calculating delay_departure, namely actual-scheduled instead of scheduled-actual\n",
    "# we therefore fix this here\n",
    "sample = sample.withColumn('delay_departure', -sample.delay_departure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f542e8bf-513e-49e4-ad31-d5c1745b82fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o sample -t df \n",
    "#writing dataframe to local to plot distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a037b0b-b529-4082-adb9-e57e2429286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if travel time is negative\n",
    "print(sample.filter(sample.travel_time<0).select('travel_time').count())\n",
    "print(sample.filter(sample.cumulated_travel_time<0).select('travel_time').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c72a2f8-b7d5-4312-b8a5-7fae6b102a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out rows with negative travel time\n",
    "sample = sample.filter(sample.travel_time>=0)\n",
    "sample = sample.filter(sample.cumulated_travel_time>=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab26fc-aebc-469a-9257-1ffaa7a9c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# plotting the continous features as histograms\n",
    "features = ['delay', 'travel_time','cumulated_travel_time','prcp', 'pres', 'snow', 'tavg', 'tmax', 'tmin', 'wdir', 'wpgt', 'wspd', 'delay_departure']\n",
    "fix, axs = plt.subplots(7,2, figsize=(24,36))\n",
    "for i,element in enumerate(features):\n",
    "    sns.histplot(sample[element], ax=axs[i//2, i%2])\n",
    "    axs[i//2, i%2].set_title(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c06e2d-13a2-4700-a7b0-243d348a2eaa",
   "metadata": {},
   "source": [
    "Notably, the delay distributions looks kinda log-normal or right-skewed distributed. Snow and precipation is mostly zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98123e9a-f33e-4ae4-a808-15bc3f921cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "# plotting the categorical variables as barplots\n",
    "features = ['transport_type', 'weekday', 'month', 'year', 'time', 'holiday']\n",
    "fix, axs = plt.subplots(3,2, figsize=(24,30))\n",
    "for i,element in enumerate(features):\n",
    "    axs[i//2, i%2].bar(sample[element].unique(), sample.groupby(element)[element].count().values)\n",
    "    axs[i//2, i%2].set_title(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da25a84-afc1-4c53-a5cb-5b36718524de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping rows with na values\n",
    "sample = sample.dropna()\n",
    "# dropping rows where transport_type is empty\n",
    "sample = sample.filter('transport_type is not null')\n",
    "sample = sample.filter('transport_type != \"\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12436a5a-20cd-41b5-9949-ed6598444311",
   "metadata": {},
   "source": [
    "#### Doing a correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78520af3-f968-4a01-b064-8e245db88ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# features to be used for correlation analysis\n",
    "features = ['delay','transport_type','stop_id', 'travel_time','cumulated_travel_time',\n",
    "            'weekday', 'month', 'year', 'time', 'prcp', 'pres', 'snow','tavg', 'tmax',\n",
    "            'tmin', 'wdir', 'wpgt', 'wspd', 'holiday', 'delay_departure']\n",
    "\n",
    "# selecting only relevant columns\n",
    "sample_2 = sample.select(features)\n",
    "\n",
    "# numeric indexing for the strings (indexing starts from 0)\n",
    "indexer = StringIndexer(inputCol=\"transport_type\", outputCol=\"t_type\")\n",
    "indexer2 = StringIndexer(inputCol=\"stop_id\", outputCol=\"s_id\")\n",
    "features.remove('transport_type')\n",
    "features.remove('stop_id')\n",
    "# creating a vector assembler\n",
    "vectorAssembler = VectorAssembler(inputCols = features + ['t_type', 's_id'], outputCol = 'features')\n",
    "\n",
    "# applying transforms\n",
    "pipeline = Pipeline(stages = [indexer, indexer2, vectorAssembler])\n",
    "vector_df = pipeline.fit(sample_2).transform(sample_2)\n",
    "vector_df = vector_df.select(['features', label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a179e-40a1-4a40-83f9-ec06cfa2fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating correlation matrix\n",
    "matrix = Correlation.corr(vector_df.select('features'), 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56727e66-2273-4b2f-929a-44a014936a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o matrix -t df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4edca3-7e99-4d71-9cb0-2e7ccaa18713",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import numpy as np\n",
    "corr_matrix = np.asarray(dict(matrix['pearson(features)']).get(0).get('values')).reshape(20,20)\n",
    "corr_delay = [element[0] for element in corr_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5536de3-1960-4d9e-96fb-ea82c1dc1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "# printing correlation between delay and the explanatory variables\n",
    "features = ['delay', 'travel_time','cumulated_travel_time', 'weekday', 'month', 'year', 'time', 'prcp', 'pres', 'snow', 'tavg', 'tmax', 'tmin', 'wdir', 'wpgt', 'wspd','holiday', 'delay_departure', 'transport_type', 'stop_id']\n",
    "for i, element in enumerate(features):\n",
    "    print(\"Correlation between %s and delay: %.3f\"%(element, corr_delay[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfedfad-18b0-41e5-bcd5-102bf2825d6b",
   "metadata": {},
   "source": [
    "Generally, there is little correlation between our explanatory variables and delay. This is worrying. Delay_departure and delay is very strongly correlated, however in production, we cannot relay on accurate estimates for delay_departure for routes that have not started yet. However, it made us realize that we must use delay_departure when we can due to the high correlation. We therefore decided to create two models. One for routes in the future, and one for routes that already have started. Stop_id also have a bit correlation, but it is difficult to use as a categorical features as it can take 2k distinct values. We therefore create a new variable below, which will indicate if the stop is one with much delay or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f9789f-3202-4da6-a8ad-e51060aaeb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# calculating number of rows per stop\n",
    "count_stops = sample.groupBy('stop_id').count().collect()\n",
    "# summing the delay per stop\n",
    "sum_delay = sample.groupBy('stop_id').sum('delay').collect()\n",
    "\n",
    "count_sum = {} # dictinaory mapping stop_id to sum_delay/count for every stop\n",
    "values = [] # values of the dictionary as list\n",
    "for i, element in enumerate(count_stops):\n",
    "    key = element[0]\n",
    "    value = sum_delay[i][1]/element[1]\n",
    "    count_sum[key] = value\n",
    "    values.append(value)\n",
    "    \n",
    "# calculating the 90th quantile of the sum_delay/count\n",
    "quant = np.quantile(np.asarray(values), 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c6552f-1ff5-4733-b45b-543120e64f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assigning 1 to stop that have top 10% delay per trip\n",
    "@F.udf\n",
    "def search(x):\n",
    "    a = count_sum.get(x)\n",
    "    if a>=quant:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "sample = sample.withColumn('delay_prone_stop', search('stop_id').cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c04eb-ae6a-4a2e-b6e3-e6a729d0455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "fig, ax = plt.subplots(1,1, figsize = (12,10))\n",
    "\n",
    "features = ['delay', 'travel_time','cumulated_travel_time', 'weekday', 'month', 'year', 'time', 'prcp', 'pres', 'snow', 'tavg', 'tmax', 'tmin', 'wdir', 'wpgt', 'wspd','holiday', 'delay_departure', 'transport_type', 'stop_id']\n",
    "\n",
    "sns.heatmap(corr_matrix, xticklabels = features, yticklabels = features, ax = ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8d66b-e56b-433b-aea8-4b45ff15bfaa",
   "metadata": {},
   "source": [
    "tavg, tmax, and tmin (avg., max and min temperature) correlates strongly with each other. We therefore only keep tavg. (as keeping all won't give more info, and to avoid near Multicollinearity in the feature matrix which will lead to numerical issues in some algorithms.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4397ebd-1932-4566-9d7b-0559f5e99e42",
   "metadata": {},
   "source": [
    "## Difference between arrival and departure times\n",
    "As previously mentioned, the predictive model we build is predicting arrival time. We therefore also need a method to predict departure times based on the delay in arrival time. We here use a simple method, namely calculate the median over the transport type in the cases where the transport is delayed in arrival such that arrival is later than scheduled departure. The reason for this method was shown above when we showed that it is very scarce that delay is created at the station, but rather betwen stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6a887-a094-4430-8917-8639ce815fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd39242-1f0f-4dba-b360-31571a487c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# calculating the difference of actual arrival and actual departure time\n",
    "# this is how long the transport is at the station\n",
    "sample_new = sample.withColumn('wait_dep', -unix_timestamp('actual_arrival_time') + unix_timestamp('actual_departure_time'))\n",
    "\n",
    "# filtering the df on rows where actual arrival > scheduled departure\n",
    "sample_new = sample_new.filter(col('actual_arrival_time') > col('scheduled_departure_time'))\n",
    "\n",
    "# function for calculating median\n",
    "magic_percentile = F.expr('percentile_approx(wait_dep, 0.5)')\n",
    "\n",
    "# calculating median over transport_type\n",
    "sample_new.groupBy('transport_type').agg(magic_percentile.alias('median_stoptime')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f2c2f-8edc-4883-a7ac-966c395ec665",
   "metadata": {},
   "source": [
    "We see our assumption is correct. When a transport vehicle is delayed, it is very quick at the stop. For bus and tram which arrived at the stop after the bus/tram was supposed to leave the stop, the median delay is only 12 seconds. For train it is 48 seconds. We therefore use these values in real time to predict departure time in the following way:\n",
    "\n",
    "pred_departure = max(scheduled_departure, pred_arrival + median_stoptime_ttype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e026fbf-5e2b-4302-9d37-527ddea51351",
   "metadata": {},
   "source": [
    "## More processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19efa748-dd4d-4863-94d8-a49ee1230acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting the values in categorical parts of the features\n",
    "print(sample.select('transport_type').distinct().collect())\n",
    "print(sample.select('weekday').distinct().collect())\n",
    "print(sample.select('month').distinct().collect())\n",
    "print(sample.select('year').distinct().collect())\n",
    "print(sample.select('time').distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb7c095-8950-42c3-b7f3-41ba85efbb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensuring categorical value index for weekday and month starts with 0 to enable one hot encoder\n",
    "# also making year a variable meaning num years since 2018, in theory should be like a time trend\n",
    "sample = sample.withColumn('weekday', sample.weekday-1)\\\n",
    "                .withColumn('month', sample.month-1)\\\n",
    "                .withColumn('year', sample.year-2018)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed1172-367e-4636-8c88-7b6bd489aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing the processed features to orc to avoid having to repeat processing every time\n",
    "sample.write.orc('/group/gutane/processed_features', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7437b92-1641-48a8-a220-4fc92833e51e",
   "metadata": {},
   "source": [
    "#### Finally modelling time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0f155-e78f-4b75-9296-9a6e97f2c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data from hdfs\n",
    "sample = spark.read.orc('/group/gutane/processed_features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd77cc69-21d2-47f2-8465-27fc8e9b3ba7",
   "metadata": {},
   "source": [
    "Small last bit of feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce1ae49-df2f-4e11-8fe4-8b7dd18ef02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column which is 1 if snow and 0 otherwise\n",
    "sample = sample.withColumn('snow2', sample.snow>0)\\\n",
    "\n",
    "# function for interacting snow2 and transport_type to a new categorical variable\n",
    "# the thesis is that ice should be more of a problem for a bus than a train\n",
    "@F.udf\n",
    "def interact_cat(x,y):\n",
    "    if x>0:\n",
    "        snow = 1\n",
    "    else:\n",
    "        snow = 0\n",
    "    if y=='Zug':\n",
    "        return snow\n",
    "    elif y=='Tram':\n",
    "        return 2+snow\n",
    "    elif y=='Bus':\n",
    "        return 4+snow\n",
    "    else:\n",
    "        return 6+snow\n",
    "\n",
    "# creating the interacted column\n",
    "sample = sample.withColumn('snow_ttype', interact_cat(sample.snow2,sample.transport_type).cast('int'))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4ad055-97c7-4fcd-a61b-e08b0f4c7d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0080630-bfa3-450e-98bb-0a5f88fdc1b5",
   "metadata": {},
   "source": [
    "#### Not one hot encoded approach\n",
    "For the random forest, we do not one hot encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8100d4-05d2-4978-bb88-4b08b0d3935f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features specify which features are to be used\n",
    "features = ['transport_type', 'cumulated_travel_time', 'weekday', 'month', 'time', 'snow_ttype']\n",
    "\n",
    "# the model will aim to predict 'delay', the residual between actual and scheduled arrival time\n",
    "label = 'delay'\n",
    "\n",
    "# non one-hot encoded approach\n",
    "indexer = StringIndexer(inputCol=\"transport_type\", outputCol=\"t_type\")\n",
    "features.remove('transport_type')\n",
    "# asssembling features to one sparse vector column\n",
    "vectorAssembler = VectorAssembler(inputCols = features + ['t_type'], outputCol = 'features')\n",
    "pipeline = Pipeline(stages = [indexer, vectorAssembler])\n",
    "non_ohe_df = pipeline.fit(sample).transform(sample)\n",
    "non_ohe_df = non_ohe_df.select(['features', label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f8090-2b53-4514-b9eb-baea97483fa6",
   "metadata": {},
   "source": [
    "#### One hot encoding\n",
    "For the rest of the models, we use one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479c5b82-2b73-4745-8284-909e4b9431a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the features column\n",
    "features = ['transport_type', 'travel_time', 'cumulated_travel_time', 'weekday', 'month', 'time', 'prcp', 'snow2', 'tavg', 'holiday', 'snow_ttype', 'delay_departure', 'delay_prone_stop']\n",
    "# the model will aim to predict 'delay', the residual between actual and scheduled arrival time\n",
    "label = 'delay'\n",
    "\n",
    "# one hot encoding appraoch\n",
    "indexer = StringIndexer(inputCol=\"transport_type\", outputCol=\"t_type\")\n",
    "features.remove('transport_type')\n",
    "oh_feat = ['weekday', 'month', 'time']\n",
    "onehotencoders = [OneHotEncoder(inputCol = element, outputCol = 'oh_'+element) for element in oh_feat]\n",
    "features.remove('weekday')\n",
    "features.remove('month')\n",
    "features.remove('time')\n",
    "# asssembling features to one sparse vector column\n",
    "vectorAssembler = VectorAssembler(inputCols = features + ['t_type','oh_weekday', 'oh_month', 'oh_time'], outputCol = 'features')\n",
    "pipeline = Pipeline(stages = [indexer]+ onehotencoders + [vectorAssembler])\n",
    "ohe_df = pipeline.fit(sample).transform(sample)\n",
    "ohe_df = ohe_df.select(['features', label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27779a7d-6ed5-4f02-b996-8aff110d311f",
   "metadata": {},
   "source": [
    "#### Splitting into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05bd5e9-367a-451c-8229-ae99dd2ca1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df = non_ohe_df # set to ohe_df for one hot encoded features\n",
    "\n",
    "# splitting the data into training and test\n",
    "splits = vector_df.randomSplit([0.9, 0.1])\n",
    "train_df = splits[0].cache()\n",
    "test_df = splits[1].cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60d5e0-680d-494a-b9eb-f9e5393f2651",
   "metadata": {},
   "source": [
    "#### Random forest regression\n",
    "First, we implement a Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b6130b-f1f8-4a3c-aafb-73a1f0db3fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 24 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=24).fit(vector_df)\n",
    "\n",
    "(trainingData, testData) = (train_df, test_df)\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\", labelCol='delay', maxDepth=10)\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, rf])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"delay\", \"features\").show(5)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"delay\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "rfModel = model.stages[1]\n",
    "print(rfModel)  # summary onl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a917a-f2fc-4c70-a520-555c6a410e78",
   "metadata": {},
   "source": [
    "#### Change of goal\n",
    "\n",
    "Currently, the model does not perform very well. It struggles to differ low and high delay. We therefore decided to change the objective of the model. We are going to discretize the delay into bins and predict in which the delay will end. This way, we have a classification task. The reason for this change is that we believe it more relevant to have a model that can tell if the bus is 8 or 0 minutes delayed than if it is 43 or 52 seconds delayed. The classification is also easier to evaluate, and the model outputs an implicit probability distribution which we can use for the confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99644127-38c5-49a9-9180-c14775cf8b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to discretize the delay variable into 10 bins\n",
    "def discretize(x):\n",
    "    # first, a bin for 0 or negative delay\n",
    "    if x<=0:\n",
    "        return 0\n",
    "    # then a bin for more than 8 minutes delay\n",
    "    elif x>8*60:\n",
    "        return 9\n",
    "    \n",
    "    #finally, every delay is binned to \n",
    "    else:\n",
    "        return x//60+1\n",
    "    \n",
    "# making udf of discretize\n",
    "disc_udf = F.udf(lambda z: discretize(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893e566c-9a84-436a-bed6-1a63e3d6ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretizing the delay variable\n",
    "vector_df = vector_df.withColumn('delay', disc_udf('delay').cast('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b18997-3d53-4d78-acfb-b9213765e98a",
   "metadata": {},
   "source": [
    "We also tried sampling the dataset to account for class imbalances, but the approach was not too succesful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d729f-3ec5-4553-8744-aa4c15c9d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# calculating the number of samples in each bin\n",
    "a = vector_df.groupBy('delay').count().sort('delay').select('count').collect()\n",
    "minimum = (np.min(np.asarray([element[0] for element in a]))) # minimum in a bin\n",
    "\n",
    "# creating a dictionary containg fractions to enable sampling to account for class imbalances\n",
    "b = [float(minimum)/float(element[0]) for element in a]\n",
    "fractions = {0:b[0], 1:b[1], 2:b[2], 3:b[3], 4:b[4], 5:b[5], 6:b[6], 7:b[7], 8:b[8], 9:b[9]}\n",
    "\n",
    "# sampling the dataset to have as many of all classes\n",
    "#vector_df = vector_df.sampleBy('delay', fractions = fractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2785613f-a870-485c-950e-6872a7246743",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df.groupBy('delay').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7d9f00-419e-4ef6-8091-a0e8a2d00072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting in training and test data\n",
    "splits = vector_df.randomSplit([0.8, 0.2])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450759b7-5237-484e-982a-28212e911236",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "\n",
    "Now, we implement a random forest classifier to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888ff2e4-0c0d-43e3-b879-c8c38fc4147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import allclose\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, MultilayerPerceptronClassifier\n",
    "\n",
    "# fitting the random forest classifier\n",
    "rf = RandomForestClassifier(numTrees=1000, maxDepth=5, labelCol='delay', seed=42, maxMemoryInMB=64)\n",
    "model = rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f99db-0b73-44cc-9a47-5ca1888fc07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Importance of feature x in the final model:')\n",
    "print('--------------------------------------------------')\n",
    "feat = features +['transport type']\n",
    "for i, element in enumerate(feat):\n",
    "    print('Feature: %s, Importance: %.3f'%(element, model.featureImportances[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398c4a48-fc5b-45a1-a178-80e5258f6814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# creating the baseline, namely predicting one for each sample\n",
    "@F.udf\n",
    "def baseline(x):\n",
    "    return 1\n",
    "\n",
    "# transforming data\n",
    "train_result = model.transform(train_df)\n",
    "# adding baseline column\n",
    "train_result = train_result.withColumn('baseline', baseline('delay').cast('double'))\n",
    "\n",
    "# creating evaluator and calculating scores for predictions\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"delay\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(train_result, {evaluator.metricName: \"accuracy\"})\n",
    "f1 = evaluator.evaluate(train_result, {evaluator.metricName: \"f1\"})\n",
    "precision = evaluator.evaluate(train_result, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(train_result, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# creating evaluator and calculating scores for baseline\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"delay\", predictionCol=\"baseline\")\n",
    "b_accuracy = evaluator.evaluate(train_result, {evaluator.metricName: \"accuracy\"})\n",
    "b_f1 = evaluator.evaluate(train_result, {evaluator.metricName: \"f1\"})\n",
    "\n",
    "# baseline is calculated as the accuracy for guessing between 0 and 1 minute delay for every prediction\n",
    "print(\"Accuracy = %.3f\" % (accuracy))\n",
    "print(\"Baseline Accuracy = %.3f\" % (b_accuracy))\n",
    "print(\"F1 = %.3f\" % (f1))\n",
    "print(\"Baseline F1 = %.3f\" % (b_f1))\n",
    "print(\"Precision = %.3f\" % (precision))\n",
    "print(\"Recall = %.3f\" % (recall))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457efac1-9e73-4080-acae-362b87d3c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting on test set\n",
    "result = model.transform(test_df)\n",
    "result.select('delay', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4c0fe6-4b42-414a-b0d4-01b2176e95d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting if the model is good in the cases where there is more than 5 min delay\n",
    "# likely, that this will be the most important examples in practice\n",
    "result.filter(result.delay>5).select('delay', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9581b-1cbe-4fca-adb8-09a8794af6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further inspecting these examples by studying the probabilities\n",
    "result.filter(result.delay>5).select('delay', 'probability').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084835bc-bd0f-478b-9020-81d11227c461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculating metrics on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"delay\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(result, {evaluator.metricName: \"accuracy\"})\n",
    "f1 = evaluator.evaluate(result, {evaluator.metricName: \"f1\"})\n",
    "precision = evaluator.evaluate(result, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(result, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "print(\"Accuracy = %.3f\" % (accuracy))\n",
    "print(\"F1 = %.3f\" % (f1))\n",
    "print(\"Precision = %.3f\" % (precision))\n",
    "print(\"Recall = %.3f\" % (recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884df98e-3f2d-4427-b802-d38187223d38",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483dc83e-823b-4d09-b3b6-e6b229f99690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# optimizing hyperparamters by cross validation\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .baseOn({rf.labelCol: 'delay'}) \\\n",
    "    .addGrid(rf.maxDepth, [4, 6, 8]) \\\n",
    "    .addGrid(rf.numTrees, [100, 500, 1000]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=5)\n",
    "\n",
    "cvModel = crossval.fit(train_df, {'labelCol':'delay'})\n",
    "\n",
    "prediction = cvModel.transform(test)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"delay\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(prediction, {evaluator.metricName: \"accuracy\"})\n",
    "f1 = evaluator.evaluate(prediction, {evaluator.metricName: \"f1\"})\n",
    "precision = evaluator.evaluate(prediction, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(prediction, {evaluator.metricName: \"weightedRecall\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbd6a9f-ea7b-4fc4-b97f-a041a3e0d2f3",
   "metadata": {},
   "source": [
    "#### Testing the models probability scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea31158-93e4-44f5-8c45-b11a6fe7ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further inspecting these examples by studying the probabilities\n",
    "num_samples = 10000\n",
    "a = result.select('delay', 'probability').take(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3bcce5-59c3-4022-bbd9-ddabcc019e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_fault = 0\n",
    "for element in a:\n",
    "    delay = element.delay\n",
    "    cum = 0\n",
    "    for i in range(delay):\n",
    "        cum+=element.probability[i]\n",
    "    \n",
    "    if cum > 0.9:\n",
    "        count_fault+=1\n",
    "\n",
    "print(float(count_fault)/num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a686a417-0479-4341-9a54-79dec8c9a744",
   "metadata": {},
   "source": [
    "## Other models\n",
    "\n",
    "We also tried implementing other models, but found the Random Forest to be the most promising and decided to down prioritize optimization of these algorithms. Please note that the feature column must be onehotencoded for lr and MLP to work properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db5e9a-f71a-487e-992f-909605759c77",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00611b1-dae5-47a0-a896-e2662522cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy: 0.426, F1 = 0.291\n",
    "lr = LogisticRegression(maxIter=30, regParam=0, elasticNetParam=0, labelCol = label)\n",
    "model = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c33fd9e-b110-4a36-9926-e8bb00cf3454",
   "metadata": {},
   "source": [
    "#### Neural Network (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0228eb84-5240-4457-b407-9b82e66b86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy 0.431\n",
    "\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 4 (features), two intermediate of size 5 and 4\n",
    "# and output of size 3 (classes)\n",
    "layers = [52, 128, 128, 12]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "#trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234, labelCol=label)\n",
    "\n",
    "# train the model\n",
    "#model = trainer.fit(train_df)\n",
    "\n",
    "# compute accuracy on the test set\n",
    "#result = model.transform(test_df)\n",
    "predictionAndLabels = result.select(\"prediction\", 'delay')\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\", labelCol='delay')\n",
    "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3131d2-5b34-4a54-9011-f9c16bc5c32d",
   "metadata": {},
   "source": [
    "### Analysis of delays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f415a94-e562-4027-a99c-104da4dd4ef0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As we decided to implement the routing algorithm locally, the inference from the predictive model must also be local. We decided to drop the ml predictive model of two reasons. 1) The model we developed was not much better than baseline. There was almost no correlation between our features and the delay. The only thing that correlated strongly was the departure at the last stop. However, we do not have access to this in real-time and can therefore not use it as a feature. If we were to make our work production-ready, we would accquire this data such that we could use real-time delays to predict future delays. 2) it was dificult to figure out how to move ml weights out of Spark and then run the model. Therefore, we rather decided to fit a probability distribution to the delays and use this as a measure of uncertainty.\n",
    "\n",
    "More information about our approaches to predictive modelling can be found in predictive_modelling.ipynb, but we present the benfits and drawbacks in short here:\n",
    "\n",
    "__Benfits__\n",
    "- Easy and simple\n",
    "- Work directly with a probability distribution which is what we try to model\n",
    "\n",
    "__Drawbacks__\n",
    "- We don't use the predictive power of the other features that we know, so it is not as accuracte as it could be\n",
    "\n",
    "If we have more time, we would use more time creating a better predictive model. It could also be interesting to incoporate real-time data into the modelling as the strongest correlator of delay is previous delay, but this is a large scope.\n",
    "\n",
    "\n",
    "In delays, there are two things we want to model: 1) How likely you are to miss a connections, and 2) How likely you are to not arrive at time at the last stop.\n",
    "\n",
    "$T_D$ = Actual time of departure\n",
    "$T_A$ = Actual time of arrival\n",
    "$S_D$ = Scheduled time of departure\n",
    "$S_A$ = Scheduled time of arrival\n",
    "$A$ = Delay in arrival time\n",
    "$D$ = Delay in departure time\n",
    "\n",
    "We then want to find for 1):\n",
    "\n",
    "$P({T_D-T_A \\geq 0 })$\n",
    "= $P({S_D + D - S_A + A \\geq 0})$\n",
    "= $P({A-D \\leq S_D - S_A})$\n",
    "\n",
    "By also accounting for transfer time are going to model:\n",
    "$P({A-D \\leq S_D - S_A-2})$\n",
    "\n",
    "which corresponds to the CDF of the random variable $A-D$. We must therefore find this distribution.\n",
    "\n",
    "In 2), we only need to model:\n",
    "\n",
    "$P({A \\leq B_T - S_A})$\n",
    "\n",
    "where $B_T$ = the demanded arrival by time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf3bf0-5f95-477c-828f-89d996bbde0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data from hdfs\n",
    "sample = spark.read.orc('/group/gutane/processed_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab999577-297a-4f5a-a91d-816fc965ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad9e06-88eb-4f06-b960-696f98c63e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7012680-16f5-4727-a3fb-68fcc5b8f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the predictive modelling notebook we have seen that transport type is what correlates the most with delay\n",
    "# and that train have different delays than bus and tram\n",
    "# therefore, we split in train and other for the model fittings\n",
    "\n",
    "# creating new column for departure delay\n",
    "sample = sample.withColumn('departure_delay', F.col('actual_departure_time').cast('long')-F.col('scheduled_departure_time').cast('long'))\n",
    "sample = sample.filter(sample.departure_delay.isNotNull())\n",
    "\n",
    "#taking a sample of departure and arrival delays\n",
    "delay_train = sample.filter(sample.transport_type=='Zug').select('delay', 'departure_delay', 'transport_type')\n",
    "delay_other = sample.filter(sample.transport_type!='Zug').select('delay', 'departure_delay', 'transport_type')\n",
    "delay_train = delay_train.sample(0.1, seed=41)\n",
    "delay_other = delay_other.sample(0.1, seed=41)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8b5b8-94bd-427d-895a-a25d67053de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o delay_train -t df -n 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41fcb0d-8ceb-4153-b741-e0fc9e4d9af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o delay_other -t df -n 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7ef0bf-336e-4a48-ae0a-c4fa7f3ed953",
   "metadata": {},
   "source": [
    "First, we model the arrival delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd35041",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(2,1,figsize=(12,12))\n",
    "sns.histplot(delay_train.delay, ax=ax[0])\n",
    "sns.histplot(delay_other.delay, ax=ax[1])\n",
    "ax[0].set_xlim(-500,1000)\n",
    "ax[0].set_title(\"Delay distribution for trains\")\n",
    "ax[1].set_xlim(-500,1000)\n",
    "ax[1].set_title(\"Delay distribution for other transport types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b735a5-7842-43f5-8e79-08858477206c",
   "metadata": {},
   "source": [
    "We observe that delay follows something that looks like a right-skewed normal distribution. The right tail is longer than the left. We therefore, try estimating the delays with a skewed normal distribution. It does also look like a log-normal distribution, but since it takes on negative values, and there is no left bound, this is not the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b41d39-8a60-4706-9dfd-30a407aba4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "from scipy import stats\n",
    "from scipy.stats import skewnorm\n",
    "\n",
    "# fitting the skewed normal distribution\n",
    "a_t, loc_t, scale_t = stats.skewnorm.fit(delay_train.delay)\n",
    "a_o, loc_o, scale_o = stats.skewnorm.fit(delay_other.delay)\n",
    "\n",
    "print(\"Estimated values\")\n",
    "print(\"a                  loc                scale\")\n",
    "print(a_t, loc_t, scale_t)\n",
    "print(a_o, loc_o, scale_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d545c-298c-461f-babe-73a33b9f2532",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import numpy as np\n",
    "fig, ax = plt.subplots(2,1,figsize=(12,12))\n",
    "sns.histplot(delay_train.delay, ax=ax[0], stat='density', label = \"empirical density\")\n",
    "sns.histplot(delay_other.delay, ax=ax[1], stat='density', label = \"empirical density\")\n",
    "\n",
    "x1 = np.linspace(skewnorm.ppf(0.00001, a_t, loc=loc_t, scale=scale_t),\n",
    "                skewnorm.ppf(0.99999, a_t, loc=loc_t, scale=scale_t), 100)\n",
    "x2 = np.linspace(skewnorm.ppf(0.00001, a_o, loc=loc_o, scale=scale_o),\n",
    "                skewnorm.ppf(0.99999, a_o, loc=loc_o, scale=scale_o), 100)\n",
    "\n",
    "ax[0].plot(x1, skewnorm.pdf(x1, a_t, loc=loc_t, scale=scale_t),\n",
    "       'r-', lw=5, alpha=0.6, label='fitted pdf')\n",
    "ax[1].plot(x2, skewnorm.pdf(x2, a_o, loc=loc_o, scale=scale_o),\n",
    "       'r-', lw=5, alpha=0.6, label='fitted pdf')\n",
    "\n",
    "ax[0].set_xlim(-500,1000)\n",
    "ax[1].set_xlim(-500,1000)\n",
    "\n",
    "ax[0].set_title(\"Distribution of delay and the fitted skewed normal pdf for trains\")\n",
    "ax[1].set_title(\"Distribution of delay and the fitted skewed normal pdf for other transportation\")\n",
    "ax[0].set_xlabel(\"Delay\")\n",
    "ax[0].set_ylabel(\"Density\")\n",
    "ax[1].set_xlabel(\"Delay\")\n",
    "ax[1].set_ylabel(\"Density\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f39a72f-2118-4f73-8ee6-030955edcc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(2,1,figsize=(12,12))\n",
    "sns.ecdfplot(delay_train.delay, ax=ax[0], lw=2, label = \"ECDF\")\n",
    "sns.ecdfplot(delay_other.delay, ax=ax[1], lw=2, label = \"ECDF\")\n",
    "\n",
    "\n",
    "x1 = np.linspace(skewnorm.ppf(0.00001, a_t, loc=loc_t, scale=scale_t),\n",
    "                skewnorm.ppf(0.99999, a_t, loc=loc_t, scale=scale_t), 100)\n",
    "x2 = np.linspace(skewnorm.ppf(0.00001, a_o, loc=loc_o, scale=scale_o),\n",
    "                skewnorm.ppf(0.99999, a_o, loc=loc_o, scale=scale_o), 100)\n",
    "\n",
    "ax[0].plot(x1, skewnorm.cdf(x1, a_t, loc=loc_t, scale=scale_t),\n",
    "       'r-', lw=2, alpha=1, label='fitted CDF')\n",
    "ax[1].plot(x2, skewnorm.cdf(x2, a_o, loc=loc_o, scale=scale_o),\n",
    "       'r-', lw=2, alpha=1, label='fitted CDF')\n",
    "\n",
    "ax[0].set_xlim(skewnorm.ppf(0.00001, a_t, loc=loc_t, scale=scale_t),\n",
    "                skewnorm.ppf(0.99999, a_t, loc=loc_t, scale=scale_t))\n",
    "ax[1].set_xlim(skewnorm.ppf(0.00001, a_t, loc=loc_t, scale=scale_t),\n",
    "                skewnorm.ppf(0.99999, a_t, loc=loc_t, scale=scale_t))\n",
    "\n",
    "ax[0].set_title(\"Empirical cumulative distribution of delay and the fitted skewed normal cdf for trains\")\n",
    "ax[1].set_title(\"Empirical cumulative distribution of delay and the fitted skewed normal cdf for other transportation\")\n",
    "ax[0].set_xlabel(\"Delay\")\n",
    "ax[0].set_ylabel(\"Density\")\n",
    "ax[1].set_xlabel(\"Delay\")\n",
    "ax[1].set_ylabel(\"Density\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f9b5f5-c7ae-4831-b7a4-b9cfe1c6c1b2",
   "metadata": {},
   "source": [
    "We see that the distribution fit the data quite good. It does quite capture the spike around the mean, but otherwise good. Especially, it seems to capture the tails which may be the most important for our task as large delays are more capable of destroying a route plan. However, a weakness in our model is that we will underestimate the number of delays between 50 and 150 seconds for other transportation. Here, there is room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a9ffd-65cf-4757-84f0-8aad6265f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "print(\"Probability that the delay is smaller than 60 seconds for trains: %.3f\"%stats.skewnorm.cdf(60,a_t, loc_t, scale_t))\n",
    "print(\"Probability that the delay is smaller than 60 seconds for other transport types: %.3f\"%stats.skewnorm.cdf(60,a_o, loc_o, scale_o))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b754f6f4-dcc1-4d88-8149-0310a1115b59",
   "metadata": {},
   "source": [
    "Then, we model the difference in arrival and departure delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d10b69d-dbcb-49f0-b0c0-3780e958d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "fig, ax = plt.subplots(2,1,figsize=(12,12))\n",
    "sns.histplot(delay_train.delay-delay_train.departure_delay, ax=ax[0])\n",
    "sns.histplot(delay_other.delay-delay_other.departure_delay, ax=ax[1])\n",
    "ax[0].set_xlim(-200,200)\n",
    "ax[0].set_title(\"Arrival minus departure delay distribution for trains\")\n",
    "ax[1].set_xlim(-200,200)\n",
    "ax[1].set_title(\"Arrival minus departure delay distribution for other transport types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02decb1-bfaa-4dd2-9afe-3695ed01caeb",
   "metadata": {},
   "source": [
    "The arrival minus distributions are harder to fit. The above one looks like some kind of Laplace distribution or similar. We tried several modelling approaches below, and found that the Laplace fittes best. For the other transport types, it is hard to find a fitting distributions, but we also here fit a Laplace distribution as we assume it should be similar as for trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6d222-c0f5-4e55-84b4-785a00e68537",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "from scipy import stats\n",
    "from scipy.stats import skewnorm, cauchy\n",
    "\n",
    "# fitting the skewed normal distribution\n",
    "a_t_stops, loc_t_stops, scale_t_stops = stats.skewnorm.fit(delay_train.delay-delay_train.departure_delay)\n",
    "a_o_stops, loc_o_stops, scale_o_stops = stats.skewnorm.fit(delay_other.delay-delay_other.departure_delay)\n",
    "\n",
    "print(\"Estimated values\")\n",
    "print(\"a                  loc                scale\")\n",
    "print(a_t, loc_t, scale_t)\n",
    "print(a_o, loc_o, scale_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a20f61-cc8f-41e4-a479-f87f47c5fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "from scipy import stats\n",
    "from scipy.stats import skewnorm, cauchy, norm, laplace\n",
    "\n",
    "# fitting the skewed normal distribution\n",
    "loc_train_laplace, scale_train_laplace = stats.laplace.fit(delay_train.delay-delay_train.departure_delay)\n",
    "loc_other_laplace, scale_other_laplace = stats.laplace.fit(delay_other.delay-delay_other.departure_delay)\n",
    "\n",
    "fig, ax = plt.subplots(2,1,figsize=(12,12))\n",
    "sns.histplot(delay_train.delay-delay_train.departure_delay, ax=ax[0], stat='density', label = \"empirical density\")\n",
    "sns.histplot(delay_other.delay-delay_other.departure_delay, ax=ax[1], stat='density', label = \"empirical density\")\n",
    "\n",
    "x1 = np.linspace(laplace.ppf(0.00001, loc_train_laplace, scale_train_laplace),\n",
    "                laplace.ppf(0.99999,loc_train_laplace, scale_train_laplace), 100)\n",
    "x2 = np.linspace(laplace.ppf(0.00001, loc_other_laplace, scale_other_laplace),\n",
    "                laplace.ppf(0.99999,  loc_other_laplace, scale_other_laplace), 100)\n",
    "\n",
    "ax[0].plot(x1, laplace.pdf(x1, loc_train_laplace, scale_train_laplace),\n",
    "       'r-', lw=5, alpha=0.6, label='fitted pdf')\n",
    "ax[1].plot(x2, laplace.pdf(x2,  loc_other_laplace, scale_other_laplace),\n",
    "       'r-', lw=5, alpha=0.6, label='fitted pdf')\n",
    "\n",
    "ax[0].set_xlim(-200,200)\n",
    "ax[1].set_xlim(-200,200)\n",
    "\n",
    "ax[0].set_title(\"Distribution of delay differences and the fitted laplace pdf for trains\")\n",
    "ax[1].set_title(\"Distribution of delay differences and the fitted laplace pdf for other transportation\")\n",
    "ax[0].set_xlabel(\"Delay\")\n",
    "ax[0].set_ylabel(\"Density\")\n",
    "ax[1].set_xlabel(\"Delay\")\n",
    "ax[1].set_ylabel(\"Density\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47198290-7e3f-4676-b602-57c4189ef71d",
   "metadata": {},
   "source": [
    "The Laplace distribution does an ok job of fitting the difference for trains, but not very much for other transportation. This is a weakness of our model, and something we would like to work more on, but we ran out of time and prioritize improving the routing algorithm. Further work should try developing a model that uses more of the available information to model the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a808a667-a887-4336-ae62-b13f01f4644f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
